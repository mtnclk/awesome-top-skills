# 标签：eval (eval)

- 返回: [标签（中文）](README.md)
- 命中技能: 25

| 技能 | 仓库 | 简介 | ⭐ | ⬇️ | 更新 | 标签 |
| - | - | - | -: | -: | - | - |
| [llm-evaluation](https://github.com/wshobson/agents/blob/main/plugins/llm-application-dev/skills/llm-evaluation/SKILL.md) | [wshobson/agents](https://github.com/wshobson/agents) | 使用自动化指标、人工反馈和基准测试评估LLM应用。 | 29K | 1.6K | 2026-02-07 | [eval (eval)](eval.md), [LLM](llm.md), [测试 (testing)](testing.md), [quality management (quality-management)](quality-management.md) |
| [skill-judge](https://github.com/softaworks/agent-toolkit/blob/main/skills/skill-judge/SKILL.md) | [softaworks/agent-toolkit](https://github.com/softaworks/agent-toolkit) | 评估代理技能设计质量，提供评分和改进建议。 | 603 | 1.6K | 2026-02-08 | [审计 (audit)](audit.md), [最佳实践 (best-practices)](best-practices.md), [eval (eval)](eval.md) |
| [eval-harness](https://github.com/affaan-m/everything-claude-code/blob/main/skills/eval-harness/SKILL.md) | [affaan-m/everything-claude-code](https://github.com/affaan-m/everything-claude-code) | 用于 Claude Code 会话的形式化评估框架，实现评估驱动开发原则。 | 47K | 365 | 2026-02-17 | [eval (eval)](eval.md), [测试 (testing)](testing.md), [代码审查 (code-review)](code-review.md), [Claude](claude.md), [LLM](llm.md) |
| [agent-evaluation](https://github.com/sickn33/antigravity-awesome-skills/blob/main/skills/agent-evaluation/SKILL.md) | [sickn33/antigravity-awesome-skills](https://github.com/sickn33/antigravity-awesome-skills) | 评估和基准测试LLM代理的行为测试、能力评估和可靠性指标。 | 10K | 239 | 2026-02-16 | [eval (eval)](eval.md), [测试 (testing)](testing.md), [智能体 (agents)](agents.md), [LLM](llm.md) |
| [scientific-critical-thinking](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/scientific/scientific-critical-thinking/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估研究严谨性，分析方法、实验设计和统计有效性。 | 21K | 228 | 2026-02-17 | [分析 (analysis)](analysis.md), [eval (eval)](eval.md), [literature review (literature-review)](literature-review.md), [scientific data (scientific-data)](scientific-data.md) |
| [agent-evaluation](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/agent-evaluation/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估和基准测试LLM代理的行为测试、能力评估和可靠性指标。 | 21K | 205 | 2026-02-17 | [eval (eval)](eval.md), [测试 (testing)](testing.md), [智能体 (agents)](agents.md), [LLM](llm.md) |
| [agentic-eval](https://github.com/github/awesome-copilot/blob/main/skills/agentic-eval/SKILL.md) | [github/awesome-copilot](https://github.com/github/awesome-copilot) | 使用自省、反思循环和LLM评判系统评估和优化AI代理输出。 | 21K | 194 | 2026-02-17 | [eval (eval)](eval.md), [智能体 (agents)](agents.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [quality management (quality-management)](quality-management.md), [测试 (testing)](testing.md) |
| [scientific-critical-thinking](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/scientific-critical-thinking/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | 评估科学主张和证据质量，用于研究和批判性分析。 | 7.8K | 125 | 2026-02-04 | [分析 (analysis)](analysis.md), [eval (eval)](eval.md), [literature review (literature-review)](literature-review.md), [academic papers (academic-papers)](academic-papers.md) |
| [evaluating-llms-harness](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-lm-evaluation-harness/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估LLM在60多个学术基准上的表现，支持HuggingFace、vLLM和API。 | 21K | 120 | 2026-02-17 | [eval (eval)](eval.md), [benchmarking (benchmarking)](benchmarking.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [huggingface (huggingface)](huggingface.md), [vllm (vllm)](vllm.md) |
| [evaluating-code-models](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-bigcode-evaluation-harness/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估代码生成模型在多个基准测试中的表现。 | 21K | 119 | 2026-02-17 | [eval (eval)](eval.md), [编码 (coding)](coding.md), [机器学习 (machine-learning)](machine-learning.md), [huggingface (huggingface)](huggingface.md), [bigcode (bigcode)](bigcode.md) |
| [scholar-evaluation](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/scholar-evaluation/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | 使用ScholarEval框架系统评估学术作品。 | 7.8K | 103 | 2026-02-04 | [eval (eval)](eval.md), [literature review (literature-review)](literature-review.md), [academic papers (academic-papers)](academic-papers.md) |
| [skill-judge](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/productivity/skill-judge/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估代理技能设计质量，提供评分和改进建议。 | 21K | 96 | 2026-02-17 | [审计 (audit)](audit.md), [最佳实践 (best-practices)](best-practices.md), [eval (eval)](eval.md) |
| [ai-evals](https://github.com/refoundai/lenny-skills/blob/main/skills/ai-evals/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | 创建和运行AI评估以衡量LLM输出质量和模型性能。 | 244 | 70 | 2026-01-31 | [eval (eval)](eval.md), [LLM](llm.md), [测试 (testing)](testing.md), [quality management (quality-management)](quality-management.md) |
| [evaluating-new-technology](https://github.com/refoundai/lenny-skills/blob/main/skills/evaluating-new-technology/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | 评估新兴技术、工具和AI供应商，用于构建与购买决策和技术架构。 | 244 | 70 | 2026-01-31 | [eval (eval)](eval.md), [架构 (architecture)](architecture.md), [最佳实践 (best-practices)](best-practices.md), [工具链 (tooling)](tooling.md) |
| [hugging-face-evaluation](https://github.com/huggingface/skills/blob/main/skills/hugging-face-evaluation/SKILL.md) | [huggingface/skills](https://github.com/huggingface/skills) | 评估和管理 Hugging Face 模型卡片，支持自动化评分和自定义评估。 | 1.2K | 69 | 2026-02-06 | [eval (eval)](eval.md), [机器学习 (machine-learning)](machine-learning.md), [huggingface (huggingface)](huggingface.md), [model 监控 (model-monitoring)](model-monitoring.md) |
| [building-with-llms](https://github.com/refoundai/lenny-skills/blob/main/skills/building-with-llms/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | 用于构建有效AI应用的技能，包括提示工程、RAG、代理设计和评估。 | 244 | 69 | 2026-01-31 | [prompting (prompting)](prompting.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [RAG（检索增强生成） (rag)](rag.md), [智能体 (agents)](agents.md), [eval (eval)](eval.md), [工具链 (tooling)](tooling.md) |
| [promptfoo-evaluation](https://github.com/daymade/claude-code-skills/blob/main/promptfoo-evaluation/SKILL.md) | [daymade/claude-code-skills](https://github.com/daymade/claude-code-skills) | 使用 Promptfoo 框架配置和运行 LLM 评估。 | 583 | 60 | 2026-02-15 | [eval (eval)](eval.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [prompting (prompting)](prompting.md), [promptfoo (promptfoo)](promptfoo.md), [LLM](llm.md) |
| [model-evaluator](https://github.com/eddiebe147/claude-settings) | [eddiebe147/claude-settings](https://github.com/eddiebe147/claude-settings) | 使用严格测试方法评估和比较ML模型性能。 | 15 | 44 | 2026-01-22 | [机器学习 (machine-learning)](machine-learning.md), [eval (eval)](eval.md), [测试 (testing)](testing.md), [model 监控 (model-monitoring)](model-monitoring.md) |
| [agent-evaluation](https://github.com/supercent-io/skills-template/blob/main/.agent-skills/agent-evaluation/SKILL.md) | [supercent-io/skills-template](https://github.com/supercent-io/skills-template) | 设计并实现AI代理的综合评估系统。 | 12 | 38 | 2026-02-13 | [智能体 (agents)](agents.md), [eval (eval)](eval.md), [LLM](llm.md) |
| [llm-evaluation](https://github.com/sickn33/antigravity-awesome-skills/blob/main/skills/llm-evaluation/SKILL.md) | [sickn33/antigravity-awesome-skills](https://github.com/sickn33/antigravity-awesome-skills) | 使用自动化指标、人工反馈和基准测试评估LLM应用。 | 10K | 36 | 2026-02-16 | [eval (eval)](eval.md), [LLM](llm.md), [测试 (testing)](testing.md), [quality management (quality-management)](quality-management.md) |
| [advanced-evaluation](https://github.com/shipshitdev/library/blob/master/skills/advanced-evaluation/SKILL.md) | [shipshitdev/library](https://github.com/shipshitdev/library) | 掌握LLM-as-a-Judge评估技术，包括直接评分、配对比较、量表生成和偏见缓解。 | 5 | 35 | 2026-02-04 | [eval (eval)](eval.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [智能体 (agents)](agents.md) |
| [evaluation](https://github.com/shipshitdev/library/blob/master/skills/evaluation/SKILL.md) | [shipshitdev/library](https://github.com/shipshitdev/library) | 构建代理系统的评估框架以测试性能和验证上下文工程选择。 | 5 | 33 | 2026-02-04 | [eval (eval)](eval.md), [测试 (testing)](testing.md), [智能体 (agents)](agents.md), [性能优化 (performance)](performance.md) |
| [nemo-evaluator-sdk](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-nemo-evaluator/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 评估LLM在100多个基准测试中的表现，支持多后端执行。 | 21K | 26 | 2026-02-17 | [eval (eval)](eval.md), [benchmarking (benchmarking)](benchmarking.md), [LLM](llm.md), [Docker (docker)](docker.md), [hpc (hpc)](hpc.md), [nvidia (nvidia)](nvidia.md) |
| [azure-ai-evaluation-py](https://github.com/microsoft/agent-skills/blob/main/.github/skills/azure-ai-evaluation-py/SKILL.md) | [microsoft/agent-skills](https://github.com/microsoft/agent-skills) | 使用 Azure AI 评估 SDK 评估生成式 AI 应用的质量、安全性和自定义评估器。 | 445 | 24 | 2026-02-03 | [eval (eval)](eval.md), [quality management (quality-management)](quality-management.md), [AI SDK (ai-sdk)](ai-sdk.md), [azure (azure)](azure.md) |
| [agent-evaluation](https://github.com/neolabhq/context-engineering-kit/blob/master/plugins/customaize-agent/skills/agent-evaluation/SKILL.md) | [neolabhq/context-engineering-kit](https://github.com/neolabhq/context-engineering-kit) | 评估并改进Claude代码命令、技能和代理的效果。 | 477 | 22 | 2026-02-16 | [eval (eval)](eval.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [智能体 (agents)](agents.md), [Claude](claude.md) |
