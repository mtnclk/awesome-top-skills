# 标签：flash attn (flash-attn)

- 返回: [标签（中文）](README.md)
- 命中技能: 2

| 技能 | 仓库 | 简介 | ⭐ | ⬇️ | 更新 | 标签 |
| - | - | - | -: | -: | - | - |
| [optimizing-attention-flash](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-flash-attention/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用Flash Attention优化Transformer注意力机制，提升推理速度并减少内存占用。 | 20K | 107 | 2026-02-14 | [LLM 优化 (llm-optimization)](llm-optimization.md), [性能优化 (performance)](performance.md), [flash attn (flash-attn)](flash-attn.md), [pytorch (pytorch)](pytorch.md), [transformers (transformers)](transformers.md) |
| [v3-performance-optimization](https://github.com/ruvnet/claude-flow/blob/main/.agents/skills/v3-performance-optimization/SKILL.md) | [ruvnet/claude-flow](https://github.com/ruvnet/claude-flow) | 全面的基准测试和优化套件，实现激进的v3性能目标，显著提升速度并减少内存占用。 | 14K | 22 | 2026-02-13 | [性能优化 (performance-optimization)](performance-optimization.md), [benchmarking (benchmarking)](benchmarking.md), [flash attn (flash-attn)](flash-attn.md), [search (search)](search.md), [memory (memory)](memory.md) |
