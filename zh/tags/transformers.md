# 标签：transformers (transformers)

- 返回: [标签（中文）](README.md)
- 命中技能: 18

| 技能 | 仓库 | 简介 | ⭐ | ⬇️ | 更新 | 标签 |
| - | - | - | -: | -: | - | - |
| [long-context](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-long-context/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用RoPE、YaRN、ALiBi和位置插值技术扩展Transformer模型上下文窗口。 | 19K | 125 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [context management (context-management)](context-management.md), [transformers (transformers)](transformers.md), [position interpolation (position-interpolation)](position-interpolation.md) |
| [knowledge-distillation](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-knowledge-distillation/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用知识蒸馏技术压缩大语言模型，实现高效部署和降低成本。 | 19K | 115 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [机器学习 (machine-learning)](machine-learning.md), [AI (ai)](ai.md), [transformers (transformers)](transformers.md) |
| [model-pruning](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-model-pruning/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用Wanda和SparseGPT等技术剪枝大型语言模型以减小尺寸并加速推理。 | 19K | 105 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [机器学习 (machine-learning)](machine-learning.md), [transformers (transformers)](transformers.md), [pytorch (pytorch)](pytorch.md) |
| [huggingface-tokenizers](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/tokenization-huggingface-tokenizers/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 高性能NLP分词器，支持BPE、WordPiece和Unigram算法。 | 19K | 100 | 2026-02-03 | [tokenization (tokenization)](tokenization.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [huggingface (huggingface)](huggingface.md), [Rust (rust)](rust.md), [transformers (transformers)](transformers.md) |
| [outlines](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/prompt-engineering-outlines/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 确保生成时的 JSON/XML/代码结构有效，支持本地模型快速推理。 | 19K | 100 | 2026-02-03 | [编码 (coding)](coding.md), [工具链 (tooling)](tooling.md), [pydantic (pydantic)](pydantic.md), [vllm (vllm)](vllm.md), [transformers (transformers)](transformers.md) |
| [peft-fine-tuning](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-peft/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用LoRA、QLoRA等方法进行高效LLM微调，适用于显存受限场景。 | 19K | 99 | 2026-02-03 | [fine tuning (fine-tuning)](fine-tuning.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [huggingface (huggingface)](huggingface.md), [transformers (transformers)](transformers.md), [gpu (gpu)](gpu.md) |
| [nanogpt](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-nanogpt/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 简洁易懂的GPT实现，用于学习Transformer架构。 | 19K | 98 | 2026-02-03 | [编码 (coding)](coding.md), [education (education)](education.md), [transformers (transformers)](transformers.md), [LLM](llm.md), [机器学习 (machine-learning)](machine-learning.md) |
| [fine-tuning-with-trl](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-trl-fine-tuning/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用TRL进行强化学习微调LLM，支持SFT、DPO、PPO和奖励建模。 | 19K | 97 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [fine tuning (fine-tuning)](fine-tuning.md), [reinforcement learning (reinforcement-learning)](reinforcement-learning.md), [huggingface (huggingface)](huggingface.md), [transformers (transformers)](transformers.md) |
| [unsloth](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-unsloth/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用 Unsloth 快速微调 LLM，训练速度提升 2-5 倍，内存占用减少 50-80%。 | 19K | 97 | 2026-02-03 | [fine tuning (fine-tuning)](fine-tuning.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [性能优化 (performance)](performance.md), [pytorch (pytorch)](pytorch.md), [transformers (transformers)](transformers.md) |
| [mamba-architecture](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-mamba/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Mamba 架构：状态空间模型，O(n) 复杂度，推理速度快 5 倍，支持百万 token 序列。 | 19K | 96 | 2026-02-03 | [架构 (architecture)](architecture.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [机器学习 (machine-learning)](machine-learning.md), [transformers (transformers)](transformers.md), [mamba (mamba)](mamba.md) |
| [optimizing-attention-flash](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-flash-attention/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 使用Flash Attention优化Transformer注意力机制，提升推理速度并减少内存占用。 | 19K | 93 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [性能优化 (performance)](performance.md), [flash attn (flash-attn)](flash-attn.md), [pytorch (pytorch)](pytorch.md), [transformers (transformers)](transformers.md) |
| [transformers](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/scientific/transformers/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 用于处理预训练Transformer模型的技能，适用于NLP、计算机视觉和多模态任务。 | 19K | 92 | 2026-02-03 | [机器学习 (machine-learning)](machine-learning.md), [natural language processing (natural-language-processing)](natural-language-processing.md), [transformers (transformers)](transformers.md), [text generation (text-generation)](text-generation.md), [fine tuning (fine-tuning)](fine-tuning.md) |
| [transformer-lens-interpretability](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/mechanistic-interpretability-transformer-lens/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 用于机制可解释性研究的TransformerLens技能。 | 19K | 91 | 2026-02-03 | [分析 (analysis)](analysis.md), [机器学习 (machine-learning)](machine-learning.md), [transformers (transformers)](transformers.md), [LLM 优化 (llm-optimization)](llm-optimization.md) |
| [gptq](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-gptq/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | 用于大型语言模型的后训练4位量化，实现最小精度损失。 | 19K | 89 | 2026-02-03 | [LLM 优化 (llm-optimization)](llm-optimization.md), [部署 (deployment)](deployment.md), [fine tuning (fine-tuning)](fine-tuning.md), [transformers (transformers)](transformers.md), [peft (peft)](peft.md) |
| [rwkv-architecture](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-rwkv/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | RWKV 是一种混合 RNN+Transformer 架构，支持线性时间和无限上下文。 | 19K | 89 | 2026-02-03 | [架构 (architecture)](architecture.md), [LLM 优化 (llm-optimization)](llm-optimization.md), [transformers (transformers)](transformers.md), [rnn (rnn)](rnn.md), [Linux foundation (linux-foundation)](linux-foundation.md) |
| [transformers](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/transformers/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | 用于处理预训练Transformer模型的技能，适用于NLP、计算机视觉和多模态任务。 | 7.8K | 72 | 2026-02-02 | [机器学习 (machine-learning)](machine-learning.md), [natural language processing (natural-language-processing)](natural-language-processing.md), [transformers (transformers)](transformers.md), [huggingface transformers (huggingface-transformers)](huggingface-transformers.md) |
| [natural-language-processing](https://github.com/aj-geddes/useful-ai-prompts/blob/main/skills/natural-language-processing/SKILL.md) | [aj-geddes/useful-ai-prompts](https://github.com/aj-geddes/useful-ai-prompts) | 使用 transformers、BERT、GPT 构建 NLP 应用，支持文本分类、命名实体识别和情感分析。 | 59 | 39 | 2025-12-28 | [natural language processing (natural-language-processing)](natural-language-processing.md), [机器学习 (machine-learning)](machine-learning.md), [text generation (text-generation)](text-generation.md), [transformers (transformers)](transformers.md), [bert (bert)](bert.md), [gpt (gpt)](gpt.md) |
| [ai-ml-timeseries](https://github.com/vasilyu1983/ai-agents-public/blob/main/frameworks/shared-skills/skills/ai-ml-timeseries/SKILL.md) | [vasilyu1983/ai-agents-public](https://github.com/vasilyu1983/ai-agents-public) | 用于时间序列预测的技能，采用现代机器学习技术如Transformer、RNN和LightGBM。 | 29 | 23 | 2026-01-26 | [机器学习 (machine-learning)](machine-learning.md), [forecasting (forecasting)](forecasting.md), [time series (time-series)](time-series.md), [深度学习](deep-learning.md), [transformers (transformers)](transformers.md), [lightgbm (lightgbm)](lightgbm.md) |
