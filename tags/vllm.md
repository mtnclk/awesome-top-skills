# Tag: vllm

- Back: [Tags](README.md)
- Tagged skills: 8

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [serving-llms-vllm](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/inference-serving-vllm/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Deploy LLM APIs with high throughput using vLLM's optimizations and support for OpenAI-compatible endpoints. | 19K | 108 | 2026-02-04 | [api](api.md), [deployment](deployment.md), [llm-optimization](llm-optimization.md), [vllm](vllm.md), [performance](performance.md) |
| [outlines](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/prompt-engineering-outlines/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Ensures valid JSON/XML/code structure during generation using Pydantic models and supports local models for fast inference. | 19K | 103 | 2026-02-04 | [coding](coding.md), [tooling](tooling.md), [pydantic](pydantic.md), [vllm](vllm.md), [transformers](transformers.md) |
| [evaluating-llms-harness](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-lm-evaluation-harness/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluates LLMs on 60+ academic benchmarks like MMLU, HumanEval, GSM8K, TruthfulQA, HellaSwag. Supports HuggingFace, vLLM, and APIs. | 19K | 100 | 2026-02-04 | [eval](eval.md), [benchmarking](benchmarking.md), [llm-optimization](llm-optimization.md), [huggingface](huggingface.md), [vllm](vllm.md) |
| [speculative-decoding](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-speculative-decoding/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Optimize LLM inference speed using speculative decoding, Medusa, and lookahead techniques for real-time applications. | 19K | 99 | 2026-02-04 | [llm-optimization](llm-optimization.md), [performance](performance.md), [deployment](deployment.md), [llm](llm.md), [vllm](vllm.md), [medusa](medusa.md) |
| [sglang](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/inference-serving-sglang/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Fast structured generation and serving for LLMs with RadixAttention prefix caching, optimized for JSON/regex outputs and agentic workflows. | 19K | 95 | 2026-02-04 | [llm-optimization](llm-optimization.md), [text-generation](text-generation.md), [tooling](tooling.md), [vllm](vllm.md), [gpu](gpu.md) |
| [openrlhf-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-openrlhf/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A high-performance RLHF framework using Ray and vLLM for large model training. | 19K | 93 | 2026-02-04 | [training](training.md), [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [ray](ray.md), [vllm](vllm.md) |
| [llamaguard](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/safety-alignment-llamaguard/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Meta's 7-8B specialized moderation model for LLM input/output filtering with 6 safety categories. | 19K | 90 | 2026-02-04 | [guardrails](guardrails.md), [moderation](moderation.md), [llm](llm.md), [huggingface](huggingface.md), [vllm](vllm.md) |
| [hqq-quantization](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-hqq/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Quantize LLMs to 4/3/2-bit precision without calibration data, suitable for fast workflows with vLLM or HuggingFace. | 19K | 89 | 2026-02-04 | [llm-optimization](llm-optimization.md), [vllm](vllm.md), [huggingface-transformers](huggingface-transformers.md) |
