# Tag: flash-attn

- Back: [Tags](README.md)
- Tagged skills: 2

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [optimizing-attention-flash](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-flash-attention/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Optimizes transformer attention using Flash Attention for faster inference and reduced memory usage. | 21K | 111 | 2026-02-17 | [llm-optimization](llm-optimization.md), [performance](performance.md), [flash-attn](flash-attn.md), [pytorch](pytorch.md), [transformers](transformers.md) |
| [v3-performance-optimization](https://github.com/ruvnet/claude-flow/blob/main/.agents/skills/v3-performance-optimization/SKILL.md) | [ruvnet/claude-flow](https://github.com/ruvnet/claude-flow) | Comprehensive benchmarking and optimization suite for achieving aggressive v3 performance targets with significant speedups and memory reduc | 14K | 26 | 2026-02-16 | [performance-optimization](performance-optimization.md), [benchmarking](benchmarking.md), [flash-attn](flash-attn.md), [search](search.md), [memory](memory.md) |
