# Tag: training

- Back: [Tags](README.md)
- Tagged skills: 5

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [training-llms-megatron](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/distributed-training-megatron-core/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Train large language models with NVIDIA Megatron-Core for high GPU efficiency and advanced parallelism. | 21K | 125 | 2026-02-17 | [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [training](training.md), [gpu](gpu.md), [megatron](megatron.md) |
| [openrlhf-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-openrlhf/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A high-performance RLHF framework using Ray and vLLM for large model training. | 21K | 102 | 2026-02-17 | [training](training.md), [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [ray](ray.md), [vllm](vllm.md) |
| [simpo-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-simpo/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A simple and efficient method for LLM preference alignment without reference models. | 21K | 97 | 2026-02-17 | [llm-optimization](llm-optimization.md), [training](training.md), [fine-tuning](fine-tuning.md) |
| [competency-builder](https://github.com/jwynia/agent-skills/blob/main/skills/general/education/competency/SKILL.md) | [jwynia/agent-skills](https://github.com/jwynia/agent-skills) | Guide competency framework development and operation for training capability building. | 22 | 37 | 2026-02-15 | [planning](planning.md), [best-practices](best-practices.md), [training](training.md) |
| [distributed-llm-pretraining-torchtitan](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-torchtitan/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Enables PyTorch-native distributed LLM pretraining via torchtitan with 4D parallelism, scaling from 8 to 512+ GPUs. | 21K | 21 | 2026-02-17 | [machine-learning](machine-learning.md), [training](training.md), [pytorch](pytorch.md), [llm](llm.md), [distributed-systems](distributed-systems.md) |
