# Tag: training

- Back: [Tags](README.md)
- Tagged skills: 4

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [training-llms-megatron](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/distributed-training-megatron-core/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Train large language models with NVIDIA Megatron-Core for high GPU efficiency and advanced parallelism. | 20K | 121 | 2026-02-16 | [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [training](training.md), [gpu](gpu.md), [megatron](megatron.md) |
| [openrlhf-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-openrlhf/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A high-performance RLHF framework using Ray and vLLM for large model training. | 20K | 100 | 2026-02-16 | [training](training.md), [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [ray](ray.md), [vllm](vllm.md) |
| [simpo-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-simpo/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A simple and efficient method for LLM preference alignment without reference models. | 20K | 95 | 2026-02-16 | [llm-optimization](llm-optimization.md), [training](training.md), [fine-tuning](fine-tuning.md) |
| [competency-builder](https://github.com/jwynia/agent-skills/blob/main/skills/general/education/competency/SKILL.md) | [jwynia/agent-skills](https://github.com/jwynia/agent-skills) | Guide competency framework development and operation for training capability building. | 21 | 37 | 2026-02-15 | [planning](planning.md), [best-practices](best-practices.md), [training](training.md) |
