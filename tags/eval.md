# Tag: eval

- Back: [Tags](README.md)
- Tagged skills: 25

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [llm-evaluation](https://github.com/wshobson/agents/blob/main/plugins/llm-application-dev/skills/llm-evaluation/SKILL.md) | [wshobson/agents](https://github.com/wshobson/agents) | Evaluate LLM applications using automated metrics, human feedback, and benchmarking. | 29K | 1.6K | 2026-02-07 | [eval](eval.md), [llm](llm.md), [testing](testing.md), [quality-management](quality-management.md) |
| [skill-judge](https://github.com/softaworks/agent-toolkit/blob/main/skills/skill-judge/SKILL.md) | [softaworks/agent-toolkit](https://github.com/softaworks/agent-toolkit) | Evaluate agent skill design quality against specifications and best practices, providing scoring and improvement suggestions for SKILL.md fi | 586 | 1.6K | 2026-02-08 | [audit](audit.md), [best-practices](best-practices.md), [eval](eval.md) |
| [eval-harness](https://github.com/affaan-m/everything-claude-code/blob/main/skills/eval-harness/SKILL.md) | [affaan-m/everything-claude-code](https://github.com/affaan-m/everything-claude-code) | An evaluation framework for Claude Code sessions implementing eval-driven development principles. | 46K | 329 | 2026-02-14 | [eval](eval.md), [testing](testing.md), [code-review](code-review.md), [claude](claude.md), [llm](llm.md) |
| [agent-evaluation](https://github.com/sickn33/antigravity-awesome-skills/blob/main/skills/agent-evaluation/SKILL.md) | [sickn33/antigravity-awesome-skills](https://github.com/sickn33/antigravity-awesome-skills) | Evaluate and benchmark LLM agents with behavioral testing, capability assessment, and reliability metrics. | 8.9K | 232 | 2026-02-14 | [eval](eval.md), [testing](testing.md), [agents](agents.md), [llm](llm.md) |
| [scientific-critical-thinking](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/scientific/scientific-critical-thinking/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluate research rigor and assess methodology, experimental design, and statistical validity for critical scientific analysis. | 20K | 216 | 2026-02-15 | [analysis](analysis.md), [eval](eval.md), [literature-review](literature-review.md), [scientific-data](scientific-data.md) |
| [agent-evaluation](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/agent-evaluation/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluate and benchmark LLM agents with behavioral testing, capability assessment, and reliability metrics. | 20K | 193 | 2026-02-15 | [eval](eval.md), [testing](testing.md), [agents](agents.md), [llm](llm.md) |
| [agentic-eval](https://github.com/github/awesome-copilot/blob/main/skills/agentic-eval/SKILL.md) | [github/awesome-copilot](https://github.com/github/awesome-copilot) | Evaluate and improve AI agent outputs using self-critique, reflection loops, and LLM-as-judge systems. | 21K | 173 | 2026-02-14 | [eval](eval.md), [agents](agents.md), [llm-optimization](llm-optimization.md), [quality-management](quality-management.md), [testing](testing.md) |
| [scientific-critical-thinking](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/scientific-critical-thinking/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | Evaluate scientific claims and evidence quality for research and critical analysis. | 7.8K | 125 | 2026-02-04 | [analysis](analysis.md), [eval](eval.md), [literature-review](literature-review.md), [academic-papers](academic-papers.md) |
| [evaluating-code-models](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-bigcode-evaluation-harness/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluates code generation models using benchmarks like HumanEval and MBPP with pass@k metrics. | 20K | 117 | 2026-02-15 | [eval](eval.md), [coding](coding.md), [machine-learning](machine-learning.md), [huggingface](huggingface.md), [bigcode](bigcode.md) |
| [evaluating-llms-harness](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-lm-evaluation-harness/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluates LLMs on 60+ academic benchmarks like MMLU, HumanEval, GSM8K, TruthfulQA, HellaSwag. Supports HuggingFace, vLLM, and APIs. | 20K | 117 | 2026-02-15 | [eval](eval.md), [benchmarking](benchmarking.md), [llm-optimization](llm-optimization.md), [huggingface](huggingface.md), [vllm](vllm.md) |
| [scholar-evaluation](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/scholar-evaluation/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | Evaluate scholarly work systematically using the ScholarEval framework across research quality dimensions. | 7.8K | 103 | 2026-02-04 | [eval](eval.md), [literature-review](literature-review.md), [academic-papers](academic-papers.md) |
| [skill-judge](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/productivity/skill-judge/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluate agent skill design quality against specifications and best practices, providing scoring and improvement suggestions for SKILL.md fi | 20K | 93 | 2026-02-15 | [audit](audit.md), [best-practices](best-practices.md), [eval](eval.md) |
| [ai-evals](https://github.com/refoundai/lenny-skills/blob/main/skills/ai-evals/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | Create and run AI evaluations for measuring LLM output quality and model performance. | 231 | 70 | 2026-01-31 | [eval](eval.md), [llm](llm.md), [testing](testing.md), [quality-management](quality-management.md) |
| [evaluating-new-technology](https://github.com/refoundai/lenny-skills/blob/main/skills/evaluating-new-technology/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | Evaluate emerging technologies, tools, and AI vendors for build vs buy decisions and technical architecture. | 231 | 70 | 2026-01-31 | [eval](eval.md), [architecture](architecture.md), [best-practices](best-practices.md), [tooling](tooling.md) |
| [hugging-face-evaluation](https://github.com/huggingface/skills/blob/main/skills/hugging-face-evaluation/SKILL.md) | [huggingface/skills](https://github.com/huggingface/skills) | Evaluate and manage Hugging Face model cards with automated scoring and custom evaluations. | 1.2K | 69 | 2026-02-06 | [eval](eval.md), [machine-learning](machine-learning.md), [huggingface](huggingface.md), [model-monitoring](model-monitoring.md) |
| [building-with-llms](https://github.com/refoundai/lenny-skills/blob/main/skills/building-with-llms/SKILL.md) | [refoundai/lenny-skills](https://github.com/refoundai/lenny-skills) | A skill for building effective AI applications, including prompt engineering, RAG, agent design, and evaluation. | 231 | 69 | 2026-01-31 | [prompting](prompting.md), [llm-optimization](llm-optimization.md), [rag](rag.md), [agents](agents.md), [eval](eval.md), [tooling](tooling.md) |
| [promptfoo-evaluation](https://github.com/daymade/claude-code-skills/blob/main/promptfoo-evaluation/SKILL.md) | [daymade/claude-code-skills](https://github.com/daymade/claude-code-skills) | Configure and run LLM evaluations using the Promptfoo framework. | 580 | 58 | 2026-02-13 | [eval](eval.md), [llm-optimization](llm-optimization.md), [prompting](prompting.md), [promptfoo](promptfoo.md), [llm](llm.md) |
| [model-evaluator](https://github.com/eddiebe147/claude-settings) | [eddiebe147/claude-settings](https://github.com/eddiebe147/claude-settings) | Evaluate and compare ML model performance using rigorous testing methods. | 15 | 44 | 2026-01-22 | [machine-learning](machine-learning.md), [eval](eval.md), [testing](testing.md), [model-monitoring](model-monitoring.md) |
| [agent-evaluation](https://github.com/supercent-io/skills-template/blob/main/.agent-skills/agent-evaluation/SKILL.md) | [supercent-io/skills-template](https://github.com/supercent-io/skills-template) | Design and implement comprehensive evaluation systems for AI agents including coding, conversational, research, and computer-use agents. | 12 | 38 | 2026-02-13 | [agents](agents.md), [eval](eval.md), [llm](llm.md) |
| [advanced-evaluation](https://github.com/shipshitdev/library/blob/master/skills/advanced-evaluation/SKILL.md) | [shipshitdev/library](https://github.com/shipshitdev/library) | Master LLM-as-a-Judge evaluation techniques including direct scoring, pairwise comparison, rubric generation, and bias mitigation. | 5 | 35 | 2026-02-04 | [eval](eval.md), [llm-optimization](llm-optimization.md), [agents](agents.md) |
| [llm-evaluation](https://github.com/sickn33/antigravity-awesome-skills/blob/main/skills/llm-evaluation/SKILL.md) | [sickn33/antigravity-awesome-skills](https://github.com/sickn33/antigravity-awesome-skills) | Evaluate LLM applications using automated metrics, human feedback, and benchmarking. | 8.9K | 33 | 2026-02-14 | [eval](eval.md), [llm](llm.md), [testing](testing.md), [quality-management](quality-management.md) |
| [evaluation](https://github.com/shipshitdev/library/blob/master/skills/evaluation/SKILL.md) | [shipshitdev/library](https://github.com/shipshitdev/library) | Build evaluation frameworks for agent systems to test performance and validate context engineering choices. | 5 | 33 | 2026-02-04 | [eval](eval.md), [testing](testing.md), [agents](agents.md), [performance](performance.md) |
| [nemo-evaluator-sdk](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/evaluation-nemo-evaluator/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Evaluates LLMs across 100+ benchmarks from 18+ harnesses with multi-backend execution on local Docker, Slurm HPC, or cloud platforms. | 20K | 24 | 2026-02-15 | [eval](eval.md), [benchmarking](benchmarking.md), [llm](llm.md), [docker](docker.md), [hpc](hpc.md), [nvidia](nvidia.md) |
| [azure-ai-evaluation-py](https://github.com/microsoft/agent-skills/blob/main/.github/skills/azure-ai-evaluation-py/SKILL.md) | [microsoft/agent-skills](https://github.com/microsoft/agent-skills) | Evaluate generative AI applications with quality, safety, and custom evaluators using Azure AI Evaluation SDK for Python. | 445 | 24 | 2026-02-03 | [eval](eval.md), [quality-management](quality-management.md), [ai-sdk](ai-sdk.md), [azure](azure.md) |
| [agent-evaluation](https://github.com/neolabhq/context-engineering-kit/blob/master/plugins/customaize-agent/skills/agent-evaluation/SKILL.md) | [neolabhq/context-engineering-kit](https://github.com/neolabhq/context-engineering-kit) | Evaluate and improve Claude Code commands, skills, and agents for better prompt effectiveness and context engineering. | 435 | 22 | 2026-02-08 | [eval](eval.md), [llm-optimization](llm-optimization.md), [agents](agents.md), [claude](claude.md) |
