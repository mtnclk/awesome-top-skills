# Tag: transformers

- Back: [Tags](README.md)
- Tagged skills: 18

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [long-context](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-long-context/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Extend transformer model context windows using RoPE, YaRN, ALiBi, and position interpolation techniques for long document processing. | 20K | 169 | 2026-02-16 | [llm-optimization](llm-optimization.md), [context-management](context-management.md), [transformers](transformers.md), [position-interpolation](position-interpolation.md) |
| [knowledge-distillation](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-knowledge-distillation/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Compress large language models using knowledge distillation techniques for efficient deployment and reduced inference costs. | 20K | 155 | 2026-02-16 | [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [ai](ai.md), [transformers](transformers.md) |
| [model-pruning](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-model-pruning/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Prune large language models to reduce size and improve inference speed using techniques like Wanda and SparseGPT. | 20K | 122 | 2026-02-16 | [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [transformers](transformers.md), [pytorch](pytorch.md) |
| [peft-fine-tuning](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-peft/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Efficient LLM fine-tuning with LoRA, QLoRA, and 25+ methods for limited GPU memory scenarios. | 20K | 120 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [huggingface](huggingface.md), [transformers](transformers.md), [gpu](gpu.md) |
| [outlines](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/prompt-engineering-outlines/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Ensures valid JSON/XML/code structure during generation using Pydantic models and supports local models for fast inference. | 20K | 118 | 2026-02-16 | [coding](coding.md), [tooling](tooling.md), [pydantic](pydantic.md), [vllm](vllm.md), [transformers](transformers.md) |
| [unsloth](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-unsloth/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Fast fine-tuning with Unsloth for LLMs, 2-5x faster training and 50-80% less memory usage. | 20K | 114 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [performance](performance.md), [pytorch](pytorch.md), [transformers](transformers.md) |
| [huggingface-tokenizers](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/tokenization-huggingface-tokenizers/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | High-performance tokenizer for NLP tasks, supporting BPE, WordPiece, and Unigram algorithms. | 20K | 113 | 2026-02-16 | [tokenization](tokenization.md), [llm-optimization](llm-optimization.md), [huggingface](huggingface.md), [rust](rust.md), [transformers](transformers.md) |
| [nanogpt](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-nanogpt/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A clean, hackable GPT implementation for learning transformer architectures. | 20K | 113 | 2026-02-16 | [coding](coding.md), [education](education.md), [transformers](transformers.md), [llm](llm.md), [machine-learning](machine-learning.md) |
| [fine-tuning-with-trl](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-trl-fine-tuning/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Fine-tune LLMs using reinforcement learning with TRL for SFT, DPO, PPO, and reward modeling. | 20K | 110 | 2026-02-16 | [llm-optimization](llm-optimization.md), [fine-tuning](fine-tuning.md), [reinforcement-learning](reinforcement-learning.md), [huggingface](huggingface.md), [transformers](transformers.md) |
| [optimizing-attention-flash](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-flash-attention/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Optimizes transformer attention using Flash Attention for faster inference and reduced memory usage. | 20K | 108 | 2026-02-16 | [llm-optimization](llm-optimization.md), [performance](performance.md), [flash-attn](flash-attn.md), [pytorch](pytorch.md), [transformers](transformers.md) |
| [transformers](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/scientific/transformers/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A skill for working with pre-trained transformer models in NLP, computer vision, and multimodal tasks. | 20K | 108 | 2026-02-16 | [machine-learning](machine-learning.md), [natural-language-processing](natural-language-processing.md), [transformers](transformers.md), [huggingface-transformers](huggingface-transformers.md) |
| [mamba-architecture](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-mamba/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Mamba architecture: State-space model with O(n) complexity, 5x faster inference, million-token sequences, no KV cache. | 20K | 105 | 2026-02-16 | [architecture](architecture.md), [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [transformers](transformers.md), [mamba](mamba.md) |
| [transformer-lens-interpretability](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/mechanistic-interpretability-transformer-lens/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A skill for mechanistic interpretability research using TransformerLens to inspect transformer internals. | 20K | 100 | 2026-02-16 | [analysis](analysis.md), [machine-learning](machine-learning.md), [transformers](transformers.md), [llm-optimization](llm-optimization.md) |
| [gptq](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-gptq/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Post-training 4-bit quantization for LLMs with minimal accuracy loss, enabling deployment of large models on consumer GPUs. | 20K | 99 | 2026-02-16 | [llm-optimization](llm-optimization.md), [deployment](deployment.md), [fine-tuning](fine-tuning.md), [transformers](transformers.md), [peft](peft.md) |
| [rwkv-architecture](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-rwkv/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | RWKV is a hybrid RNN+Transformer architecture with O(n) inference, supporting linear time and infinite context. | 20K | 94 | 2026-02-16 | [architecture](architecture.md), [llm-optimization](llm-optimization.md), [transformers](transformers.md), [rnn](rnn.md), [linux-foundation](linux-foundation.md) |
| [transformers](https://github.com/k-dense-ai/claude-scientific-skills/blob/main/scientific-skills/transformers/SKILL.md) | [k-dense-ai/claude-scientific-skills](https://github.com/k-dense-ai/claude-scientific-skills) | A skill for working with pre-trained transformer models in NLP, computer vision, and multimodal tasks. | 7.8K | 78 | 2026-02-04 | [machine-learning](machine-learning.md), [natural-language-processing](natural-language-processing.md), [transformers](transformers.md), [huggingface-transformers](huggingface-transformers.md) |
| [natural-language-processing](https://github.com/aj-geddes/useful-ai-prompts/blob/main/skills/natural-language-processing/SKILL.md) | [aj-geddes/useful-ai-prompts](https://github.com/aj-geddes/useful-ai-prompts) | Build NLP applications using transformers, BERT, GPT, text classification, named entity recognition, and sentiment analysis. | 73 | 39 | 2025-12-28 | [natural-language-processing](natural-language-processing.md), [machine-learning](machine-learning.md), [text-generation](text-generation.md), [transformers](transformers.md), [bert](bert.md), [gpt](gpt.md) |
| [ai-ml-timeseries](https://github.com/vasilyu1983/ai-agents-public/blob/main/frameworks/shared-skills/skills/ai-ml-timeseries/SKILL.md) | [vasilyu1983/ai-agents-public](https://github.com/vasilyu1983/ai-agents-public) | A skill for time series forecasting using modern ML techniques like Transformers, RNNs, and LightGBM with emphasis on explainability and lon | 30 | 23 | 2026-01-26 | [machine-learning](machine-learning.md), [forecasting](forecasting.md), [time-series](time-series.md), [deep-learning](deep-learning.md), [transformers](transformers.md), [lightgbm](lightgbm.md) |
