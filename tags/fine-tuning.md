# Tag: fine-tuning

- Back: [Tags](README.md)
- Tagged skills: 14

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [fine-tuning-expert](https://github.com/jeffallan/claude-skills/blob/main/skills/fine-tuning-expert/SKILL.md) | [jeffallan/claude-skills](https://github.com/jeffallan/claude-skills) | Optimize LLM performance through fine-tuning, parameter-efficient methods, and dataset preparation. | 2.7K | 211 | 2026-02-13 | [llm-optimization](llm-optimization.md), [machine-learning](machine-learning.md), [fine-tuning](fine-tuning.md) |
| [model-merging](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-model-merging/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Merge multiple fine-tuned models using mergekit for enhanced performance and experimentation. | 20K | 129 | 2026-02-16 | [llm-optimization](llm-optimization.md), [modeling](modeling.md), [fine-tuning](fine-tuning.md), [machine-learning](machine-learning.md), [deployment](deployment.md) |
| [peft-fine-tuning](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-peft/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Efficient LLM fine-tuning with LoRA, QLoRA, and 25+ methods for limited GPU memory scenarios. | 20K | 120 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [huggingface](huggingface.md), [transformers](transformers.md), [gpu](gpu.md) |
| [unsloth](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-unsloth/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Fast fine-tuning with Unsloth for LLMs, 2-5x faster training and 50-80% less memory usage. | 20K | 114 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [performance](performance.md), [pytorch](pytorch.md), [transformers](transformers.md) |
| [llama-factory](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-llama-factory/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A skill for fine-tuning LLMs using LLaMA-Factory with WebUI, supporting 100+ models and QLoRA. | 20K | 113 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [multimodal](multimodal.md), [huggingface](huggingface.md), [llm](llm.md) |
| [fine-tuning-with-trl](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-trl-fine-tuning/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Fine-tune LLMs using reinforcement learning with TRL for SFT, DPO, PPO, and reward modeling. | 20K | 110 | 2026-02-16 | [llm-optimization](llm-optimization.md), [fine-tuning](fine-tuning.md), [reinforcement-learning](reinforcement-learning.md), [huggingface](huggingface.md), [transformers](transformers.md) |
| [implementing-llms-litgpt](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/model-architecture-litgpt/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Implement and train LLMs using LitGPT with 20+ architectures like Llama, Gemma, Phi, Qwen, Mistral. | 20K | 104 | 2026-02-16 | [coding](coding.md), [llm-optimization](llm-optimization.md), [fine-tuning](fine-tuning.md), [llm](llm.md), [pytorch](pytorch.md) |
| [axolotl](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/fine-tuning-axolotl/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Guidance for fine-tuning LLMs with Axolotl using YAML configs, LoRA/QLoRA, DPO/KTO/ORPO/GRPO, and multimodal support. | 20K | 100 | 2026-02-16 | [fine-tuning](fine-tuning.md), [llm-optimization](llm-optimization.md), [multimodal](multimodal.md), [yaml](yaml.md), [lora](lora.md) |
| [gptq](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/optimization-gptq/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Post-training 4-bit quantization for LLMs with minimal accuracy loss, enabling deployment of large models on consumer GPUs. | 20K | 99 | 2026-02-16 | [llm-optimization](llm-optimization.md), [deployment](deployment.md), [fine-tuning](fine-tuning.md), [transformers](transformers.md), [peft](peft.md) |
| [grpo-rl-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-grpo-rl-training/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Expert guidance for GRPO/RL fine-tuning with TRL for reasoning and task-specific model training. | 20K | 99 | 2026-02-16 | [fine-tuning](fine-tuning.md), [reinforcement-learning](reinforcement-learning.md), [llm-optimization](llm-optimization.md), [trl](trl.md), [grpo](grpo.md) |
| [simpo-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/post-training-simpo/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | A simple and efficient method for LLM preference alignment without reference models. | 20K | 95 | 2026-02-16 | [llm-optimization](llm-optimization.md), [training](training.md), [fine-tuning](fine-tuning.md) |
| [hugging-face-model-trainer](https://github.com/huggingface/skills/blob/main/skills/hugging-face-model-trainer/SKILL.md) | [huggingface/skills](https://github.com/huggingface/skills) | Train or fine-tune language models using Hugging Face Jobs infrastructure with TRL methods. | 1.2K | 87 | 2026-02-06 | [machine-learning](machine-learning.md), [fine-tuning](fine-tuning.md), [huggingface](huggingface.md), [trl](trl.md), [gguf](gguf.md), [gpu](gpu.md) |
| [fine-tuning-assistant](https://github.com/eddiebe147/claude-settings) | [eddiebe147/claude-settings](https://github.com/eddiebe147/claude-settings) | A guide for fine-tuning language models to achieve customized AI performance. | 15 | 45 | 2026-01-22 | [llm-optimization](llm-optimization.md), [fine-tuning](fine-tuning.md), [machine-learning](machine-learning.md), [model-monitoring](model-monitoring.md) |
| [ai-llm](https://github.com/vasilyu1983/ai-agents-public/blob/main/frameworks/shared-skills/skills/ai-llm/SKILL.md) | [vasilyu1983/ai-agents-public](https://github.com/vasilyu1983/ai-agents-public) | A production LLM engineering skill covering strategy selection, dataset design, PEFT/LoRA, evaluation workflows, and deployment. | 30 | 23 | 2026-01-26 | [llm-optimization](llm-optimization.md), [prompting](prompting.md), [rag](rag.md), [fine-tuning](fine-tuning.md), [evaluation](evaluation.md), [deployment](deployment.md) |
