# Tag: deepspeed

- Back: [Tags](README.md)
- Tagged skills: 2

| Skill | Repo | Summary | ⭐ | ⬇️ | Updated | Tags |
| - | - | - | -: | -: | - | - |
| [deepspeed](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/distributed-training-deepspeed/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Expert guidance for distributed training with DeepSpeed including ZeRO optimization and FP16/BF16 support. | 19K | 108 | 2026-02-04 | [machine-learning](machine-learning.md), [llm-optimization](llm-optimization.md), [deepspeed](deepspeed.md), [distributed-systems](distributed-systems.md) |
| [moe-training](https://github.com/davila7/claude-code-templates/blob/main/cli-tool/components/skills/ai-research/emerging-techniques-moe-training/SKILL.md) | [davila7/claude-code-templates](https://github.com/davila7/claude-code-templates) | Train Mixture of Experts (MoE) models using DeepSpeed or HuggingFace for large-scale, sparse architectures. | 19K | 105 | 2026-02-04 | [machine-learning](machine-learning.md), [deepspeed](deepspeed.md), [huggingface](huggingface.md) |
